1))HDFS
hadoop distributed file system.
• The file store in HDFS provides scalable, fault-tolerant storage at low cost
• The HDFS software detects and compensates for hardware issues, including disk
problems and server failure
• HDFS stores files across the collection of servers in a cluster
• Files are decomposed into blocks and each block is written to more than one of
the servers
• The replication provides both fault-tolerance and performance

2))Hadoop cluster

* Data Storage has grown exponentially in the recent past,
 but data reading speed has not improved radically
*A Hadoop cluster is a special type of computational cluster 
designed specifically for storing and analyzing huge amounts 
of unstructured data in a distributed computing environment. 
*Hadoop clusters are known for boosting the speed of data analysis
 applications. They also are highly scalable.
* If a cluster's processing power is overwhelmed by growing volumes
 of data, additional cluster nodes can be added to increase throughput.
* Hadoop clusters also are highly resistant to failure because each piece 
of data is copied onto other cluster nodes, which ensures that the data is 
not lost if one node fails.

3)) HDFS Blocks

• Hard Disk has concentric circles which form tracks
• One file can contain many blocks
• These blocks in a local file system are nearly
512 bytes and are not necessarily continuous
• For HDFS, since it is designed for large files, block
size is 128 MB by default
• Moreover, it gets blocks of local file system
contiguously to minimise head seek time